{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVR,SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_imputation_numeric(train_numeric):\n",
    "    \"\"\"\n",
    "    Impute numeric data using MICE imputation with Gradient Boosting Regressor.\n",
    "    (we can use any other regressors to impute the data)\n",
    "    \"\"\"\n",
    "    iter_imp_numeric = IterativeImputer(LGBMRegressor())\n",
    "    imputed_train = iter_imp_numeric.fit_transform(train_numeric)\n",
    "    train_numeric_imp = pd.DataFrame(imputed_train, columns = train_numeric.columns, index= train_numeric.index)\n",
    "    return train_numeric_imp\n",
    "\n",
    "def mice_imputation_categoric(train_categoric):\n",
    "    \"\"\"\n",
    "    Impute categoric data using MICE imputation with Gradient Boosting Classifier.\n",
    "    Steps:\n",
    "    1. Ordinal Encode the non-null values\n",
    "    2. Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n",
    "    (we can use any other classifier to impute the data)\n",
    "    3. Inverse transform the ordinal encoded data.\n",
    "    \"\"\"\n",
    "    ordinal_dict={}\n",
    "    for col in train_categoric:\n",
    "        '''Ordinal encode train data'''\n",
    "        ordinal_dict[col] = OrdinalEncoder()\n",
    "        nn_vals = np.array(train_categoric[col][train_categoric[col].notnull()]).reshape(-1,1)\n",
    "        nn_vals_arr = np.array(ordinal_dict[col].fit_transform(nn_vals)).reshape(-1,)\n",
    "        train_categoric[col].loc[train_categoric[col].notnull()] = nn_vals_arr\n",
    "\n",
    "    '''Impute the data using MICE with LightGBM '''\n",
    "    iter_imp_categoric = IterativeImputer(LGBMClassifier(), max_iter =5, initial_strategy='most_frequent')\n",
    "    imputed_train = iter_imp_categoric.fit_transform(train_categoric)\n",
    "    train_categoric_imp = pd.DataFrame(imputed_train, columns =train_categoric.columns,index = train_categoric.index).astype(int)\n",
    "    \n",
    "    '''Inverse Transform'''\n",
    "    for col in train_categoric_imp.columns:\n",
    "        oe = ordinal_dict[col]\n",
    "        train_arr= np.array(train_categoric_imp[col]).reshape(-1,1)\n",
    "        train_categoric_imp[col] = oe.inverse_transform(train_arr)\n",
    "        \n",
    "    return train_categoric_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_encoding(cdata):\n",
    "    le=LabelEncoder()\n",
    "    cat_col_mask=cdata.columns\n",
    "    for col in cat_col_mask:\n",
    "        le.fit(cdata[col].unique().tolist())\n",
    "        cdata[col]=le.transform(cdata[col])\n",
    "    return cdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputemissing(impdata):\n",
    "    if (impdata.dtypes.name=='object'):\n",
    "        categoric_cols = impdata.select_dtypes('object').columns\n",
    "        impdata[categoric_cols].fillna(impdata[categoric_cols].mode()[0],inplace=True)\n",
    "        impdata[categoric_cols]=dep_encoding(impdata[categoric_cols])\n",
    "    if (impdata.dtypes.name=='int64' or impdata.dtypes.name=='float64'):\n",
    "        numeric_cols = impdata.select_dtypes(['float64','int64']).columns\n",
    "        impdata[numeric_cols].fillna(impdata[categoric_cols].median(),inplace=True)\n",
    "    return impdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inde_encoding(idata):\n",
    "    le=LabelEncoder()\n",
    "    le.fit(idata.unique().tolist())\n",
    "    idata=le.transform(idata)\n",
    "    return idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x1):\n",
    "    col=x1.columns\n",
    "    features = x1[col]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    x1[col] = features\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For normal and balanced dataset\n",
    "def normalsplit(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,random_state = 42,test_size=0.20)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For imbalance dataset\n",
    "def imbsplit(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,random_state = 42,shuffle=True,test_size=0.20,stratify=y)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_reg_metrics(y_test, y_pred):\n",
    "    r2=metrics.r2_score(y_true = y_test, y_pred = y_pred)\n",
    "    mse=metrics.mean_squared_error(y_true = y_test, y_pred = y_pred)\n",
    "    mae=metrics.mean_absolute_error(y_true = y_test, y_pred = y_pred)\n",
    "    rmse=np.sqrt(mse)\n",
    "    \n",
    "    R2_score.append(r2)\n",
    "    MSE.append(mse)\n",
    "    MAE.append(mae)\n",
    "    RMSE.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_class_metrics(y_test,y_pred):\n",
    "    acc = metrics.accuracy_score(y_true = y_test, y_pred = y_pred)\n",
    "    pre = metrics.precision_score(y_true = y_test, y_pred = y_pred)\n",
    "    rec = metrics.recall_score(y_true = y_test, y_pred = y_pred)\n",
    "    f1  = metrics.f1_score(y_true = y_test, y_pred = y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    log_loss = metrics.log_loss(y_true = y_test, y_pred = y_pred)\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    precision.append(pre)\n",
    "    recall.append(rec)\n",
    "    f1_scor.append(f1)\n",
    "    roc_auc_scor.append(roc_auc)\n",
    "    logLoss.append(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kfold(model,x,y):\n",
    "    #skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=54)\n",
    "    cvs = cross_val_score(model, x, y, cv = 20)\n",
    "    cvs_mean.append(cvs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, X_train):\n",
    "\n",
    "    fI = model.feature_importances_\n",
    "    \n",
    "    print(fI)\n",
    "    \n",
    "    names = x_train.columns.values\n",
    "    \n",
    "    ticks = [i for i in range(len(names))]\n",
    "    \n",
    "    plt.bar(ticks, fI)\n",
    "    \n",
    "    plt.xticks(ticks, names,rotation = 90)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_model(x,y):\n",
    "    x_train,x_test,y_train,y_test=normalsplit(x,y)\n",
    "    LR=LinearRegression()\n",
    "    LA=Lasso()\n",
    "    RI=Ridge()\n",
    "    DTR=DecisionTreeRegressor()\n",
    "    KNR=KNeighborsRegressor()\n",
    "    LSVR=LinearSVR()\n",
    "    ETR=ExtraTreesRegressor()\n",
    "    ABR=AdaBoostRegressor()\n",
    "    RFR=RandomForestRegressor()\n",
    "    GBR=GradientBoostingRegressor()\n",
    "    lgbr=LGBMRegressor()\n",
    "    xgbr=xgb.XGBRegressor()\n",
    "    catbost=CatBoostRegressor(silent=True)\n",
    "    \n",
    "    models=[]\n",
    "    models.append((\"LinearRegression\",LR))\n",
    "    models.append((\"Lasso\",LA))\n",
    "    models.append((\"Ridge\",RI))\n",
    "    models.append((\"DecisionTreeRegressor\",DTR))\n",
    "    models.append((\"LinearSVR\",LSVR))\n",
    "    models.append((\"ExtraTreeRegressor\",ETR))\n",
    "    models.append((\"AdaBoostRegressor\",ABR))\n",
    "    models.append((\"RandomForestRegressor\",RFR))\n",
    "    models.append((\"GradientBoostingRegressor\",GBR))\n",
    "    models.append(('LGBMRegressor',lgbr))\n",
    "    models.append(('XGBRegressor',xgbr))\n",
    "    models.append(('CatBoostRegressor',catbost))\n",
    "    \n",
    "    Model = []\n",
    "    for name,model in models:\n",
    "        Model.append(name)\n",
    "        model.fit(x_train,y_train)\n",
    "        pre=model.predict(x_test)\n",
    "        cal_reg_metrics(y_test, pre)\n",
    "        calculate_kfold(model,x,y)\n",
    "    \n",
    "    results = pd.DataFrame(data = {'R2':R2_score, 'MSE': MSE ,'MAE': MAE,'kfold_mean': cvs_mean, \"RMSE\":RMSE}, \n",
    "                           index = Model )\n",
    "    print(tabulate(results, headers =results.columns, tablefmt = 'fancy_grid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cla_model(x,y):\n",
    "    x_train,x_test,y_train,y_test=imbsplit(x,y)\n",
    "    \n",
    "    LR=LogisticRegression()\n",
    "    SV=SVC()\n",
    "    KNN=KNeighborsClassifier()\n",
    "    GNB=GaussianNB()\n",
    "    RFC=RandomForestClassifier()\n",
    "    DTC=DecisionTreeClassifier()\n",
    "    ETC=ExtraTreesClassifier()\n",
    "    GBC=GradientBoostingClassifier()\n",
    "    ABC=AdaBoostClassifier()\n",
    "    lgbc=LGBMClassifier()\n",
    "    xgbc=xgb.XGBClassifier()\n",
    "    catbost=CatBoostClassifier(silent=True)\n",
    "    \n",
    "    models = []\n",
    "    models.append(('KNeighborsClassifier', KNN))\n",
    "    models.append(('SVC', SV))\n",
    "    models.append(('LogisticRegression', LR))\n",
    "    models.append(('GaussianNB', GNB))\n",
    "    models.append(('RandomForestClassifier', RFC))\n",
    "    models.append(('DecisionTreeClassifier',DTC))\n",
    "    models.append(('ExtraTreesClassifier',ETC))\n",
    "    models.append(('GradientBoostingClassifier', GBC))\n",
    "    models.append(('AdaBoostClassifier',ABC))\n",
    "    models.append(('LGBMClassifier',lgbc))\n",
    "    models.append(('XGBClassifier',xgbc))\n",
    "    models.append(('CatboostClassifier',catbost))\n",
    "    \n",
    "    Model = []\n",
    "    for name,model in models:\n",
    "        Model.append(name)\n",
    "        model.fit(x_train,y_train)\n",
    "        pre=model.predict(x_test)\n",
    "        cal_class_metrics(y_test, pre)\n",
    "        cvs=calculate_kfold(model,x,y)\n",
    "    \n",
    "    results = pd.DataFrame(data = {'accuracy':accuracy, 'precision': precision,'recall': recall,'f1_score':f1_scor,\n",
    "                                   'kfold_mean': cvs, \"Roc_auc_scor\":roc_auc_scor, \"Log_Loss\":logLoss}, \n",
    "                           index = Model )\n",
    "    print(tabulate(results, headers = results.columns, tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentMissingFeature(data):\n",
    "    data_na = (data.isnull().sum() / len(data)) * 100\n",
    "    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "    print(tabulate(missing_data, headers = ['features','Missing%'], tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataflow1(df,dependent):\n",
    "    percentMissingFeature(df)\n",
    "    data=df.drop([dependent],axis=1)\n",
    "    y=df[dependent]\n",
    "    data=imputemissing(data)\n",
    "    if(np.any(data.dtypes=='object')):\n",
    "        categoric_cols = data1.select_dtypes('object').columns\n",
    "        data[categoric_cols] =dep_encoding(data[categoric_cols])\n",
    "    x=scale(data)\n",
    "    if((y.dtype.name == 'int64') and (np.any(y.values>=2))):\n",
    "        reg_model(x,y)\n",
    "    if((y.dtype.name == 'int64') and (np.any(y.values<=2))):\n",
    "        cla_model(x,y)\n",
    "    if(y.dtype.name == 'object'):\n",
    "        y=inde_encoding(y)\n",
    "        cla_model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the dependent variableviews\n",
      "╒════════════╤════════════╕\n",
      "│ features   │ Missing%   │\n",
      "╞════════════╪════════════╡\n",
      "╘════════════╧════════════╛\n",
      "╒═══════════════════════════╤════════════╤══════════════════╤═════════╤══════════════╤══════════╕\n",
      "│                           │         R2 │              MSE │     MAE │   kfold_mean │     RMSE │\n",
      "╞═══════════════════════════╪════════════╪══════════════════╪═════════╪══════════════╪══════════╡\n",
      "│ LinearRegression          │  0.0739078 │      1.01541e+06 │ 573.942 │   -0.0248501 │ 1007.68  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ Lasso                     │  0.0742934 │      1.01499e+06 │ 573.899 │   -0.0238987 │ 1007.47  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ Ridge                     │  0.0739168 │      1.0154e+06  │ 573.95  │   -0.0247852 │ 1007.67  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ DecisionTreeRegressor     │ -0.531589  │      1.67931e+06 │ 481.355 │   -0.771918  │ 1295.88  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ LinearSVR                 │ -0.0813435 │      1.18564e+06 │ 411.206 │   -0.0754965 │ 1088.87  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ ExtraTreeRegressor        │  0.250109  │ 822218           │ 405.873 │   -0.317897  │  906.763 │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ AdaBoostRegressor         │ -0.209117  │      1.32574e+06 │ 840.542 │   -0.435271  │ 1151.41  │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ RandomForestRegressor     │  0.328215  │ 736579           │ 409.72  │   -0.139739  │  858.242 │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ GradientBoostingRegressor │  0.155053  │ 926442           │ 497.653 │    0.081243  │  962.519 │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ LGBMRegressor             │  0.201456  │ 875564           │ 479.223 │    0.115655  │  935.716 │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ XGBRegressor              │  0.170178  │ 909859           │ 453.461 │   -0.169591  │  953.865 │\n",
      "├───────────────────────────┼────────────┼──────────────────┼─────────┼──────────────┼──────────┤\n",
      "│ CatBoostRegressor         │  0.185408  │ 893160           │ 465.979 │   -0.03112   │  945.071 │\n",
      "╘═══════════════════════════╧════════════╧══════════════════╧═════════╧══════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1_scor = []\n",
    "roc_auc_scor = []\n",
    "logLoss = []\n",
    "cvs_mean=[]\n",
    "R2_score=[]\n",
    "cvs=[]\n",
    "MSE=[]\n",
    "MAE=[]\n",
    "RMSE=[]\n",
    "REMSE=[]\n",
    "df=pd.read_csv('train_meta_df.csv')\n",
    "dep=input(\"Enter the dependent variable\")\n",
    "dataflow1(df,dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
